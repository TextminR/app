---
title: "topic-modelling"
author: "Elisa Bankl"
date: "2023-01-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load all packages that are used later on. 
```{r}
library(tm)
library(topicmodels)
library(quanteda)
library(quanteda.textmodels)
library(text2vec)
library(seededlda)
library(lda)
library(LDAvis)
library(LSAfun)
```

In this Notebook, the package tm is used to create a corpus and a Document Term Matrix. This matrix is then used as input to the Latent Semantic Analysis and Latent Dirichlet Allocation functions of the packages listed above. 
In the case of topicModels and lsa, the Document Term Matrix of the tm package can be used as an input directly. 
The packages quanteda, lda and seededlda work with the Document Feature Matrix and teh text2vec functions require a sparse Matrix as input.
The packages LDAvis and LSAfun are used to visualize results of the LDA and LSA functions.






## Read in the dataset

```{r}
textdata <- base::readRDS(url("https://slcladal.github.io/data/sotu_paragraphs.rda", "rb"))
head(textdata)
```

## Create a tm Corpus

```{r}
corpus <- tm::VCorpus(DataframeSource(textdata))
```

## Set the corpus to lower case, remove stopwords, remove punctuation and 
## stem the words using the package tm

```{r}
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus
                          , removeWords, stopwords())
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)
```

## create a document term matrix, leaving out very rare words

```{r}
DTM <- tm::DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(5, Inf))))
```


# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata

```{r}
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]
```

# LSA 

## LSA can only handle small Document term matrices

```{r}
lsa_DTM = DTM[sample(ncol(DTM),100),] 
sel_idx <- slam::col_sums(lsa_DTM) > 0
lsa_DTM <- lsa_DTM[,sel_idx]
```


```{r}
lsa_lsa <- lsa::lsa(lsa_DTM,20)
```




# LSAfun

## Visualize the neighbors of a word based on an lsa space


```{r}
LSAfun::neighbors("island",10,lsa_lsa$dk)
```
```{r}
LSAfun::plot_neighbors("island",10,lsa_lsa$dk)
```

# topicModel

```{r}
topicModel_lda <- topicmodels::LDA(DTM, 40, method="Gibbs", control=list(iter=400)) #only 400 iterations to make it faster
```


## topicModel Visualisierung

```{r}
# the code is from here: https://gist.github.com/trinker/477d7ae65ff6ca73cace
topicmodels2LDAvis <- function(x, ...){
    post <- topicmodels::posterior(x)
    if (ncol(post[["topics"]]) < 3) stop("The model must contain > 2 topics")
    mat <- x@wordassignments
    LDAvis::createJSON(
        phi = post[["terms"]], 
        theta = post[["topics"]],
        vocab = colnames(post[["terms"]]),
        doc.length = slam::row_sums(mat, na.rm = TRUE),
        term.frequency = slam::col_sums(mat, na.rm = TRUE)
    )
}
```



```{r}
LDAvis::serVis(topicmodels2LDAvis(topicModel_lda))
```

## text2vec

## convert the DTM to the format text2vec needs

```{r}
#the code is from here: https://stackoverflow.com/questions/49835762/convert-documenttermmatrix-to-dgtmatrix
text2vec_DTM <-  Matrix::sparseMatrix(i=DTM$i, 
                           j=DTM$j, 
                           x=DTM$v, 
                           dims=c(DTM$nrow, DTM$ncol),
                           dimnames = DTM$dimnames)
```


## text2vec lsa


```{r}
text2vec_lsa = text2vec::LSA$new(n_topics = 40)
doc_embeddings =  fit_transform(text2vec_DTM,text2vec_lsa)
```

```{r}
LSAfun::plot_neighbors("island",10,tvectors=t(text2vec_lsa$components))
```


## text2vec lda

```{r}
text2vec_lda = text2vec::LDA$new(n_topics = 40, doc_topic_prior = 0.1, topic_word_prior = 0.01)
```


```{r}
doc_embeddings =  fit_transform(text2vec_DTM, text2vec_lda)
```


## text2vec lda Visualization

```{r}
text2vec_lda$plot()
```

# quanteda


## convert from Document Term Matrix (DTM) to Document Feature Matrix (DFM)


```{r}
library(tidytext)
tidy(DTM) %>%
  cast_dfm(document,term,count) -> DFM
```

## quanteda_lsa

```{r}
quanteda_lsa = quanteda.textmodels::textmodel_lsa(DFM,40)
```

## visualize quanteda_lsa

```{r}
LSAfun::plot_neighbors("island",10,tvectors=quanteda_lsa$features)
```

# seededlda

## seeedlda also needs a Document Feature Matrix instead of a Document Term Matrix

```{r}
seededlda_lda = seededlda::textmodel_lda(DFM,40)
```



```{r}
seededlda_lda = seededlda::textmodel_lda(DFM,40)
```


## visualize seededlda

```{r}
seededlda2LDAvis <- function(x, ...){

    if (ncol(x$phi) < 3) stop("The model must contain > 2 topics")
    
    LDAvis::createJSON(
        phi = x$phi, 
        theta = x$theta,
        vocab = colnames(x$phi),
        doc.length = slam::row_sums(x$data, na.rm = TRUE),
        term.frequency = slam::col_sums(x$data, na.rm = TRUE)
    )
}
```

```{r}
LDAvis::serVis(seededlda2LDAvis(seededlda_lda))
```

# lda

```{r}
lda_input=topicmodels::dtm2ldaformat(DTM)
lda_lda = lda::lda.cvb0(lda_input$documents,K=40,lda_input$vocab,num.iterations = 40,alpha=0.1,eta=0.01)
```